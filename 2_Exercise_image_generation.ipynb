{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coriander4all/exercicegan/blob/main/2_Exercise_image_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLUnjsLdA7O9"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BqlMlWFn1lgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13bbf59-b343-4656-daa8-38501b0db01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.15.0\n",
            "Uninstalling tensorflow-2.15.0:\n",
            "  Successfully uninstalled tensorflow-2.15.0\n",
            "Collecting tensorflow==2.15\n",
            "  Using cached tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.2.2)\n",
            "Using cached tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oChX5IitAyXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c18ffa4-36ea-4fe3-f005-f778d2fb73bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.11/dist-packages (3.49.0)\n",
            "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (3.1.0)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (4.23.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (5.9.5)\n",
            "Requirement already satisfied: python-box<7.0.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (6.1.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.32.3)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.20.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.11/dist-packages (from comet_ml) (3.19.3)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (1.14.1)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (3.1.1)\n",
            "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (0.22.7)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (13.9.4)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.11/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (5.0.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.22.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet_ml) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install comet_ml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poKWk6yV_-MR"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvxCMSK65g-H"
      },
      "source": [
        "TODO: Signup for a free account at https://www.comet.ml and paste your API key below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d9AkuK4V_9Ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298ee080-00f1-48fe-ec9e-60e12c5dd59f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/coriandering/applied-ai-gan-students/fb2ea26eb11a4dccb216e26ccc7d7d6c\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import Comet package for tracking the experiments\n",
        "\n",
        "from comet_ml import Experiment\n",
        "### TODO: Signup for a free account at https://www.comet.ml and paste your API key below\n",
        "exp = Experiment(\n",
        "    api_key=\"WT6qlzLB67kzqU40wHG3vhkJG\",\n",
        "    project_name='applied-ai-gan-students')\n",
        "### END OF TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xnjbpUZ0JAG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348c037b-9383-499e-9893-0cfef7dcb72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
          ]
        }
      ],
      "source": [
        "# Import models, layers, optmisers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Reshape, Conv2DTranspose, Conv2D, Flatten, LeakyReLU, ReLU,BatchNormalization\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import cifar10, mnist\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import initializers\n",
        "#other packages\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from skimage.transform import resize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_URgg8xexel-"
      },
      "source": [
        "## Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p8dxlWYcxdif"
      },
      "outputs": [],
      "source": [
        "# Globals variables :\n",
        "noise_input_shape = 100\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WRDdcycGMD5"
      },
      "source": [
        "## Utils functions (plotting, logging)\n",
        "Take a look at TODOs ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Czc4tKc2GVBy"
      },
      "outputs": [],
      "source": [
        "# Function to plot generated images :\n",
        "'''\n",
        "Arguments :\n",
        "epoch - training epoch\n",
        "generator - generator model from epoch epoch\n",
        "exp - Comet.ml callback\n",
        "examples - number of images to be plotted\n",
        "dim - (rows, cols) of figure in which images will be plotted\n",
        "figsize - size of the figure\n",
        "prefix - prefix for the filename of the saved plot\n",
        "'''\n",
        "def plot_generated_images(epoch, generator, exp, examples=25, dim=(5, 5), figsize=(5, 5), prefix='dataset'):\n",
        "    # TODO: Random input vector (shape=(num_examples,noise_input_shape))\n",
        "    noise = tf.random.normal(shape=(examples, noise_input_shape))\n",
        "    # TODO: Generate images from created noise vector\n",
        "    generated_images = generator(noise)\n",
        "    # Convert images into range [0-255] and convert to uint8 for proper plotting\n",
        "    generated_images = ((generated_images * 127.5) + 127.5).astype('uint8')\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(np.squeeze(generated_images[i]))\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(prefix+'_gan_generated_image %d.png' % epoch)\n",
        "    # Log plotted figure into Comet.ml\n",
        "    exp.log_figure(figure_name='generated_images_ep'+str(epoch))\n",
        "    plt.close()\n",
        "\n",
        "# Function to plot real images :\n",
        "'''\n",
        "Arguments :\n",
        "X_train - real dataset images (or batch of real dataset images)\n",
        "exp - Comet.ml callback\n",
        "examples - number of images to be plotted\n",
        "dim - (rows, cols) of figure in which images will be plotted\n",
        "figsize - size of the figure\n",
        "prefix - prefix for the filename of the saved plot\n",
        "\n",
        "'''\n",
        "def plot_real_images(X_train, exp, examples=25, dim=(5, 5), figsize=(5, 5), prefix='dataset'):\n",
        "    print(X_train.shape)\n",
        "    real_images = X_train[:examples]\n",
        "    print(real_images.shape)\n",
        "    real_images = ((real_images.numpy() * 127.5) + 127.5).astype('uint8')\n",
        "    print(real_images.shape)\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(real_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(np.squeeze(real_images[i]))\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(prefix+'_gan_real_image %d.png')\n",
        "    # Log plotted figure into Comet.ml\n",
        "    exp.log_figure(figure_name='real_images.png')\n",
        "    plt.close()\n",
        "\n",
        "# Comet logging gradinets distributions (vanishing gradients)\n",
        "def get_gradients(gradmap, grads, model):\n",
        "    for grad, param in zip(grads, model.trainable_variables):\n",
        "        gradmap.setdefault(param.name, 0)\n",
        "        gradmap[param.name] += grad\n",
        "\n",
        "    return gradmap\n",
        "def log_histogram(experiment, gradmap, step, prefix=None):\n",
        "    for k, v in gradmap.items():\n",
        "        experiment.log_histogram_3d(v, name=\"%s/%s\" % (prefix, k), step=step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmcCofYMuEVs"
      },
      "source": [
        "##  Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JfPOSsLGuSrq"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset='mnist' #'cifar'\n",
        "if dataset=='mnist':\n",
        "  (x_train, y_train), (_, _) = mnist.load_data()\n",
        "  out_channels = 1\n",
        "  x_train = x_train[...,np.newaxis] #?\n",
        "elif dataset=='cifar':\n",
        "  (x_train, y_train), (_, _) = cifar10.load_data()\n",
        "  out_channels = 3\n",
        "else:\n",
        "  sys.exit('Unknown dataset')\n",
        "\n",
        "# Normalise data to the range (-1,1)\n",
        "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "y_train = y_train.astype(np.int32)\n",
        "\n",
        "# Training details\n",
        "batch_count = x_train.shape[0] // batch_size # how many iteration per epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v-efEUF9vPnb"
      },
      "outputs": [],
      "source": [
        "# If dataset is MNIST, we will resize the images to 32x32 size because 32 is more convenient being a power of 2.\n",
        "# CIFAR images are already of size 32x32, resizing will change nothing\n",
        "resize_shape = (32, 32, out_channels)\n",
        "\n",
        "def parse_function(img):\n",
        "    image = tf.image.resize(img,(resize_shape[0],resize_shape[1]))\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ni5Q7Z7HuWEO"
      },
      "outputs": [],
      "source": [
        "# make trainig set\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dataset = train_dataset.shuffle(x_train.shape[0])\n",
        "train_dataset = train_dataset.map(parse_function, num_parallel_calls=4)\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for t in train_dataset:\n",
        "#  print(t.shape)\n",
        "#  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOtUWUQG9U95",
        "outputId": "44323517-5317-49e1-afb2-bc904c9e44b3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 32, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggd3EM5t_0JE"
      },
      "source": [
        "## **GAN model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idTZyfVHsmsR"
      },
      "source": [
        "### Generator :\n",
        "\n",
        "TODO - Generator transforms input noise to image. The input noise is reshaped to 3D tensor and upsampling is done via TransposeConvolution (take a look at https://keras.io/api/layers/convolution_layers/convolution2d_transpose/).\n",
        "\n",
        "Complete the generator to have the following architecture:\n",
        "\n",
        "<img src=\"https://seafile.unistra.fr/f/92b1617dc1724e39abd4/?dl=1\"/>\n",
        "\n",
        "\n",
        "*Input*: Noise (shape=(noise_input_shape,)).\n",
        "\n",
        "*Output*: Image (32,32,1) for MNIST or (32,32,3) for CIFAR.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eN3iElnXtn9_"
      },
      "outputs": [],
      "source": [
        "# TODO: Function to create generator\n",
        "def create_generator(noise_input_shape=100,out_channels=3):\n",
        "  # Input noise is a vector of length 100 (for example)\n",
        "  input = Input(shape=(noise_input_shape,))\n",
        "  filters = 256\n",
        "  k_size = 4\n",
        "  g = Reshape((1,1,noise_input_shape))(input)\n",
        "  g = Conv2DTranspose(filters,(k_size,k_size),strides=1,kernel_initializer=initializers.RandomNormal(stddev=0.02),bias_initializer=initializers.RandomNormal(stddev=0.02))(g)\n",
        "  g = LeakyReLU(alpha=0.2)(g)\n",
        "\n",
        "  ### TODO: complete the model (see https://keras.io/layers/convolutional/), all leakyReLu layers to have alpha=0.2, 4x4 convolution, and (2,2) stride\n",
        "  # Upsampling blocks\n",
        "  for i in range(1,4):\n",
        "    f = filters//(2**i)\n",
        "    # Spatial upsampling\n",
        "    g = Conv2DTranspose(f, (4,4), strides=(2,2), padding=\"same\", kernel_initializer=initializers.RandomNormal(stddev=0.02), bias_initializer=initializers.RandomNormal(stddev=0.02))(g)\n",
        "    g = LeakyReLU(alpha=0.2)(g)\n",
        "\n",
        "  # N.B. Number of channels (filters) in the last layer needs to be 1 for MNIST (output should be grayscale image),\n",
        "  # or 3 for CIFAR (output should be an RGB image), with 3x3 convolution\n",
        "  # Activation function in last layer should be tanh to match real images which are in the range [-1,1]\n",
        "  g = Conv2D(out_channels, (3,3), padding=\"same\", activation='tanh')(g)\n",
        "  ### END OF TODO\n",
        "\n",
        "  # Make the model\n",
        "  model = Model(inputs=input, outputs=g)\n",
        "\n",
        "  # Model doesn't need to be compiled because it will be trained in custom training\n",
        "  # loop in combination with discriminator (new model will be created)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a77iU1Xiw8Tp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21c7205b-32a7-4aa3-a0f9-0476185bf22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 1, 1, 100)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTr  (None, 4, 4, 256)         409856    \n",
            " anspose)                                                        \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2D  (None, 8, 8, 128)         524416    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2D  (None, 16, 16, 64)        131136    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2D  (None, 32, 32, 32)        32800     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 1)         289       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1098497 (4.19 MB)\n",
            "Trainable params: 1098497 (4.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x78b06469f8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Test generator\n",
        "g = create_generator(out_channels=out_channels)\n",
        "g.summary()\n",
        "noise = tf.random.normal([1,noise_input_shape])\n",
        "gen_img = g(noise,training=False).numpy()\n",
        "plot_img = np.squeeze(gen_img)\n",
        "plt.imshow(((plot_img * 127.5) + 127.5).astype('uint8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IP5lZKus_6_"
      },
      "source": [
        "### Discriminator:\n",
        "\n",
        "TODO - Discriminator distinguishing between real and fake samples:\n",
        "It takes an image as an input and produce the probability of a given image being real.\n",
        "This can be acheived  by using sigmoid activation function at the last layer. For computational stability, use **linear activation** in last layer (no activation) and **sigmoid will be added during loss calculation**.\n",
        "\n",
        "\n",
        "Complete the discriminator to have the following architecture:\n",
        "\n",
        "<img src=\"https://seafile.unistra.fr/f/2739577c4dde45cca672/?dl=1\"/>\n",
        "\n",
        "\n",
        "*Input*: Image (32,32,1) for MNIST or (32,32,3) for CIFAR.\n",
        "\n",
        "*Output*: logit of binary classification (real or fake), shape=(1,))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zxutqe3Ps_c2"
      },
      "outputs": [],
      "source": [
        "# TODO: Function to create discriminator\n",
        "def create_discriminator(input_shape):\n",
        "    # Discriminator input has the same shape as the generator output\n",
        "    input = Input(input_shape)\n",
        "    filters = 64\n",
        "    k_size= 3\n",
        "    d = Conv2D(filters, (k_size, k_size),padding='same')(input)\n",
        "    d = LeakyReLU(alpha=0.2)(d)\n",
        "\n",
        "    ### TODO: complete the model - all convolutional layers to have a stride of (2,2), all leakyReLu layers to have alpha=0.2\n",
        "    for i in range(1,4):\n",
        "      f = filters*(2 ** i)\n",
        "      d = Conv2D(f, (4,4), strides=(2,2), padding=\"same\", kernel_initializer=initializers.RandomNormal(stddev=0.02), bias_initializer=initializers.RandomNormal(stddev=0.02))(d)\n",
        "      d = LeakyReLU(alpha=0.2)(d)\n",
        "\n",
        "\n",
        "    d = Flatten()(d)\n",
        "    # Output is one number\n",
        "    # Ideal output should be close to 1 for images from real dataset, close to 0 for generated images\n",
        "    d = Dense(1)(d)\n",
        "    ### END OF TODO\n",
        "\n",
        "    model = Model(inputs = input,outputs=d)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EwLOVR7kzlTi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8705c3d3-7313-4792-c761-fe70133bffb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        640       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 256)         524544    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 4, 4, 512)         2097664   \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2762241 (10.54 MB)\n",
            "Trainable params: 2762241 (10.54 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "tf.Tensor([[0.05922379]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Test discriminator\n",
        "d = create_discriminator(resize_shape)\n",
        "d.summary()\n",
        "decision = d(gen_img)\n",
        "print(decision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "NeODLUTsOXYH",
        "outputId": "d07e49e6-5910-483a-8ecb-cf2c38195d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.05882967]], shape=(1, 1), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x78b061ba97d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Run this multiple times to see differences\n",
        "noise = tf.random.normal([1,noise_input_shape])\n",
        "gen_img = g(noise,training=False).numpy()\n",
        "decision = d(gen_img)\n",
        "print(decision)\n",
        "plot_img = np.squeeze(gen_img)\n",
        "plt.imshow(((plot_img* 127.5) + 127.5).astype('uint8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e34ZO8lktJzu"
      },
      "source": [
        "### Loss functions and optimisers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX3jZHvoNo35"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N6zkZRVdGEOF"
      },
      "outputs": [],
      "source": [
        "#TODO: Create binary cross entropy loss function (use parameter from_logits = True if activation is linear in disciminator).\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
        "\n",
        "#TODO: (hint: disciminator should predict one for real data)\n",
        "def discriminator_loss_real_data(disc_pred_for_real_data):\n",
        "    real_loss = loss_function(disc_pred_for_real_data, tf.ones(disc_pred_for_real_data.shape))\n",
        "    return real_loss\n",
        "\n",
        "#TODO: (hint: disciminator should  predict zero for real data)\n",
        "def discriminator_loss_fake_data(disc_pred_for_generated_data):\n",
        "    fake_loss = loss_function(disc_pred_for_generated_data, tf.zeros(disc_pred_for_generated_data.shape))\n",
        "    return fake_loss\n",
        "\n",
        "#TODO: (hint: generator wants that discriminator's predictions are close to one for generated data)\n",
        "def generator_loss(disc_pred_for_generated_data):\n",
        "    gen_loss =  loss_function(disc_pred_for_generated_data, tf.ones(disc_pred_for_generated_data.shape))\n",
        "    return gen_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RZKf3rH9GSud"
      },
      "outputs": [],
      "source": [
        "#Optimisers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AO41Sahyupp"
      },
      "source": [
        "# Prepare models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZsXh9XUNjpv"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "r8sz1UD5aQ55"
      },
      "outputs": [],
      "source": [
        "#TODO: Make models\n",
        "generator = create_generator(out_channels=out_channels)\n",
        "shape = generator.output_shape[1:]\n",
        "discriminator = create_discriminator(resize_shape)\n",
        "\n",
        "# Log and plot progress (Comet.ml)\n",
        "exp.get_callback('keras').set_model(generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ajfgM5Gl5P"
      },
      "source": [
        "# GAN training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj8cLgaXzJlD"
      },
      "source": [
        "## Training step function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntspLjmuNgwE"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "febylNJVGosR"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(real_images):\n",
        "  ## Train discriminator\n",
        "  #0. Make a noise vector\n",
        "  noise = tf.random.normal([batch_size,noise_input_shape]) #tf.random.normal(real_images.shape)\n",
        "  with tf.GradientTape() as disc_tape:\n",
        "    # 1. generator generates an image from noise\n",
        "    generated_images = generator(noise)\n",
        "    # 2. discriminator prediction for real images\n",
        "    real_output = discriminator(real_images)\n",
        "    # 3. discriminator prediction for fake images\n",
        "    fake_output = discriminator(generated_images)\n",
        "    # 4. discriminator loss: (loss_on_real+loss_on_fake)/2\n",
        "    disc_loss = (discriminator_loss_fake_data(fake_output)+discriminator_loss_real_data(real_output))/2\n",
        "\n",
        "  #get discriminator's gradients\n",
        "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "  # Update the weights of the discriminator using the discriminator optimizer\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "  ## Train generator\n",
        "  #0. Make a noise vector\n",
        "  noise = tf.random.normal([batch_size,noise_input_shape])\n",
        "  with tf.GradientTape() as gen_tape:\n",
        "      # 1. Generate fake images using the generator\n",
        "      generated_images =  generator(noise)\n",
        "      # 2. Get the discriminator prediction for fake images\n",
        "      fake_output = discriminator(generated_images)\n",
        "      # 3. Calculate the generator loss\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "\n",
        "  #TODO: Update generator (see update discriminator)\n",
        "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "\n",
        "  return gen_loss, disc_loss, gradients_of_generator, gradients_of_discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH1h0ZBTzaGB"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "S0WRXgCPJkCc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(dataset, epochs):\n",
        "  real_plt = False\n",
        "  cnt = 0\n",
        "  gradmap_generator = {}\n",
        "  gradmap_discriminator = {}\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    step=0\n",
        "    for image_batch in dataset:\n",
        "      #print(\"a\")\n",
        "      step = step+1\n",
        "      if not real_plt:\n",
        "         print(\"b\")\n",
        "         plot_real_images(image_batch, exp, prefix='real')\n",
        "         real_plt = True\n",
        "\n",
        "      print(\"c\")\n",
        "      gen_loss, disc_loss, gradients_of_generator, gradients_of_discriminator= train_step(image_batch)\n",
        "      print(\"d\")\n",
        "\n",
        "      # Logs for comet\n",
        "      gradmap_generator = get_gradients(gradmap_generator, gradients_of_generator, generator)\n",
        "      gradmap_discriminator = get_gradients(gradmap_discriminator, gradients_of_discriminator, discriminator)\n",
        "\n",
        "      exp.set_step(cnt)\n",
        "      exp.log_metric(\"disc_loss\",disc_loss,step=cnt)\n",
        "      exp.log_metric(\"gen_loss\", gen_loss, step=cnt)\n",
        "      cnt = cnt+1\n",
        "     # scale gradients\n",
        "    for k, v in gradmap_generator.items():\n",
        "      gradmap_generator[k] = v / step\n",
        "    for k, v in gradmap_discriminator.items():\n",
        "      gradmap_discriminator[k] = v / step\n",
        "\n",
        "    plot_generated_images(epoch, generator, exp, prefix='cifar')\n",
        "    log_histogram(exp, gradmap_generator,cnt, prefix=\"gradient_generator\")\n",
        "    log_histogram(exp, gradmap_discriminator,cnt, prefix=\"gradient_discriminator\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac4hKGrU0365"
      },
      "source": [
        "##Run code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD6_QSzEJS0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9fe5139-887a-4764-a507-48fe1b483de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\n",
            "(128, 32, 32, 1)\n",
            "(25, 32, 32, 1)\n",
            "(25, 32, 32, 1)\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n",
            "d\n",
            "c\n"
          ]
        }
      ],
      "source": [
        "train(train_dataset, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnGOBybys6VU"
      },
      "outputs": [],
      "source": [
        "exp.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6N3_ujS8i43"
      },
      "source": [
        "# GAN playground\n",
        "\n",
        "1. Try longer training\n",
        "1. Train GAN with CIFAR dataset\n",
        "1.  Change the learning rate of the Generator/Discriminator, try different optimisers\n",
        "2.  Change Input noise shape\n",
        "3.   Modify discriminator/generator architecture (adding BatchNormalisarion, deeper generator/discriminator)\n",
        "4.   Increase the number of discriminator training iterations relative to the generator per epoch, e.g. discriminator is trained 2 times and generator 1 time per epoch\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "R6N3_ujS8i43"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}